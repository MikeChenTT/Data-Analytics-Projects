{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Modeling Boston House Prices with Feature Selection\n"},{"metadata":{},"cell_type":"markdown","source":"The Boston House Prices dataset is a great one to get one's feet wet on regression models. The objective is to build the best model to predict median house price based on thirteen features, such as crime rate, accessibility to highways, and property tax rate. This is a classic multivariate regression problem. As you will see later on, the biggest challenge we try to solve when modeling the data is **multicollinearity**, and we will solve it toward the end."},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='white')\n%matplotlib inline\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import the dataset from sklearn library\nfrom sklearn.datasets import load_boston\ndata = load_boston()\nprint(data.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load features, and target variable, combine them into a single dataset\nX = pd.DataFrame(data.data, columns=data.feature_names)\n# Add constant \nX = sm.add_constant(X)\ny = pd.Series(data.target, name='MEDV')\ndataset = pd.concat([X, y], axis=1)\n\n# Split training and test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Print dataset description\nprint(data.DESCR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we run couple basic functions to make sure the dataset was loaded correctly."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Exploration\nBefore we implement any algorithms, let's go ahead and explore the dataset."},{"metadata":{},"cell_type":"markdown","source":"First, let's take a look at the distribution of the target variable-Median house price."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.distplot(dataset['MEDV'], rug=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph is a combination of histogram, kernel density estimate, and plot of every single data point. From the graph, we can see that the distribution of the target variable is pretty close to a normal distribution with a few outliers to the right."},{"metadata":{},"cell_type":"markdown","source":"Now let's take a look at the variables and how correlated they are with each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.zeros_like(dataset.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(dataset.corr(), annot=True, vmin=-1, vmax=1, square=True, mask=mask, cmap=cmap, linewidths=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph above, we can see that there is quite amount of strong and close to strong correlations happening among feature variables, especially between TAX (full-value property-tax rate per $10,000) and RAD (index of accessibility to radial highways). This causes problems when fitting linear models since linear models have assumed no or little multicollinearity.\n\nWe will take care of that as we train our model."},{"metadata":{},"cell_type":"markdown","source":"## Modeling with Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"We will train a multivariate linear regression to model the dataset mapping from features to median house price. Because we have recognized from the last graph that the features are not linearly independent from each other, after every time when we fit a model, we will check the VIF (variance inflation factor) for multicollinearity, and take the feature with the largest off the model. And once every VIF value is lower than 5, we arrive at a model with low enough multicollinearity.\n\nAnd it is important to notice that a model built on high multicollinearity dataset, although may fit the current dataset with high accuracy, is not a useful model.\n\nAfter implementing the model with constant and without constant, I found there to be a huge difference in the model generated and the R2 values, so I include both implementations down below."},{"metadata":{},"cell_type":"markdown","source":"### Without Constant X0"},{"metadata":{},"cell_type":"markdown","source":"Here we will run through the first iteration of model training, and we will implement a while loop afterwards."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a linear model to data\nexog = X_train.drop('const', axis=1)\nendog = y_train\nmodel = sm.OLS(endog, exog).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in warning that there is a strong multicollinearity in the dataset. Let's check the VIF values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking VIF\nvariables = model.model.exog\nvif = np.array([variance_inflation_factor(variables, i) for i in range(variables.shape[1])])\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case we will take out the feature with the highest VIF value, which is PTRATIO."},{"metadata":{},"cell_type":"markdown","source":"Now let's write our while loop implementation of our feature selection algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"exog = X_train.drop('const', axis=1)\nendog = y_train\n\n# Fit model\nmodel = sm.OLS(endog, exog).fit()\n# Calculate VIF\nvariables = model.model.exog\nvif = np.array([variance_inflation_factor(variables, i) for i in range(variables.shape[1])])\nvif[0] = 0\n\nwhile sum(np.array(vif) > 5) > 0:\n    # Find the id of the feature with largest VIF value\n    max_vif_id = np.argmax(vif)\n\n    # Delete that feature from exog dataset\n    exog = exog.drop(exog.columns[max_vif_id], axis=1)\n    \n    # Fit model again\n    model = sm.OLS(endog, exog).fit()\n    # Calculate VIF\n    variables = model.model.exog\n    vif = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n    vif[0] = 0\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we arrived at our model with low enough multicollinearity, we can make a more concise model by taking out features with P-value above 0.05 iteratively."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if any P-value larger or equal to 0.05\nwhile sum(model.pvalues >= 0.05) > 0:\n    # Find the index of the feature of the largest p-value\n    max_pvalue_id = np.argmax(model.pvalues)\n\n    # Delete that feature from exog dataset\n    exog = exog.drop(max_pvalue_id, axis=1)\n    \n    # Fit model again\n    model = sm.OLS(endog, exog).fit()\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the long process of feature selection, we arrived at our most significant and linearly independent model with four features. The model uses CRIM (per capita crime rate), CHAS (if the area bounds the Charles River), DIS (weighted distances to five Boston employment centres), and RAD (index of accessibility to radial highways) as independent variables to predict MEDV (median house price). The multivariate linear model without any further tuning for polynomial terms achieves an adjusted R-squared score of 0.791 on the training set."},{"metadata":{},"cell_type":"markdown","source":"### With Constant X0\n"},{"metadata":{},"cell_type":"markdown","source":"Here we will run through the first iteration of model training, and we will implement a while loop afterwards.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a linear model to data\nexog = X_train\nendog = y_train\nmodel = sm.OLS(endog, exog).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in warning that there is a strong multicollinearity in the dataset. Let's check the VIF values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking VIF\nvariables = model.model.exog\nvif = np.array([variance_inflation_factor(variables, i) for i in range(variables.shape[1])])\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the constant has the highest VIF value, which is quite interesting. In this case we will take out the feature with the highest VIF value other than the constant, which is TAX."},{"metadata":{},"cell_type":"markdown","source":"Now let's write our while loop implementation of our feature selection algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"exog = X_train\nendog = y_train\n\n# Fit model\nmodel = sm.OLS(endog, exog).fit()\n# Calculate VIF\nvariables = model.model.exog\nvif = np.array([variance_inflation_factor(variables, i) for i in range(variables.shape[1])])\nvif[0] = 0\n\nwhile sum(np.array(vif) > 5) > 0:\n    # Find the id of the feature with largest VIF value\n    max_vif_id = np.argmax(vif)\n\n    # Delete that feature from exog dataset\n    exog = exog.drop(exog.columns[max_vif_id], axis=1)\n    \n    # Fit model again\n    model = sm.OLS(endog, exog).fit()\n    # Calculate VIF\n    variables = model.model.exog\n    vif = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n    vif[0] = 0\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we arrived at our model with low enough multicollinearity, we can make a more concise model by taking out features with P-value above 0.05 iteratively."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if any P-value larger or equal to 0.05\nwhile sum(model.pvalues >= 0.05) > 0:\n    # Find the index of the feature of the largest p-value\n    max_pvalue_id = np.argmax(model.pvalues)\n\n    # Delete that feature from exog dataset\n    exog = exog.drop(max_pvalue_id, axis=1)\n    \n    # Fit model again\n    model = sm.OLS(endog, exog).fit()\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we include the constant term in the model, the features selected are drastically different, and the R2 value is quite different now as well."},{"metadata":{},"cell_type":"markdown","source":"## Going Forward\n"},{"metadata":{},"cell_type":"markdown","source":"1. For the two models, we need to test on the test sets for performance.\n2. Understanding why the constant has such high VIF value and if it's ok to drop the constant\n3. A decision tree or other non-linear model could be used at modeling this dataset, so we don't need to deal with multicollinearity ourselves."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}