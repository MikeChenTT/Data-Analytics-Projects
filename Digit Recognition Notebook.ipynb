{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Digit Recognition 99.185%"},{"metadata":{},"cell_type":"markdown","source":"The MNIST dataset is the de facto \"hello world\" dataset of computer vision. It contains images of handwritten digits 0-9. In this notebook, we will take a look at some of the data, preprocess it, and build models for classifying the digits. "},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/test.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Import Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Separate the training dataset into training and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train.iloc[:, 1:], train.iloc[:, 0], test_size=0.2)\nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\nprint('X_val:', X_val.shape)\nprint('y_val:', y_val.shape)\n","execution_count":4,"outputs":[{"output_type":"stream","text":"X_train: (33600, 784)\ny_train: (33600,)\nX_val: (8400, 784)\ny_val: (8400,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at some data."},{"metadata":{"trusted":true},"cell_type":"code","source":"@interact\ndef show_digital(x=(0, 1000)):\n    return plt.imshow(np.array(X_train.iloc[x]).reshape((28,28))), print('The number is', y_train.iloc[x])\n","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=500, description='x', max=1000), Output()), _dom_classes=('widget-intera…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0db9fd424e4294816de61a8e6c9148"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"The only preprocessing we will do here is to normalize the dataset to distribution of [0,1], and turn the Pandas data structure into Numpy arrays."},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalization(X):\n    X = X / 255.0\n    return X","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(normalization(X_train))\nX_val = np.array(normalization(X_val))\ny_train = np.array(y_train)\ny_val = np.array(y_val)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{},"cell_type":"markdown","source":"We will run couple classification algorithms on the dataset, and train the best one on the entire training set (train+val) to predict the test set."},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\nprint('Score:', LR.score(X_val, y_val))","execution_count":8,"outputs":[{"output_type":"stream","text":"Score: 0.9158333333333334\nCPU times: user 12.7 s, sys: 84 ms, total: 12.8 s\nWall time: 12.9 s\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machines (SVM)\nhttps://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.svm import LinearSVC\nSVM = LinearSVC().fit(X_train, y_train)\nprint('Score:', SVM.score(X_val, y_val))","execution_count":9,"outputs":[{"output_type":"stream","text":"Score: 0.9067857142857143\nCPU times: user 54 s, sys: 120 ms, total: 54.2 s\nWall time: 54.2 s\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### K-Nearest Neighbors (KNN)\nhttps://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(X_train, y_train)\nprint('Score:', KNN.score(X_val, y_val))","execution_count":10,"outputs":[{"output_type":"stream","text":"Score: 0.9665476190476191\nCPU times: user 11min 37s, sys: 1.43 s, total: 11min 39s\nWall time: 6min 5s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Convolutional Neural Network (CNN)\nhttps://keras.io/layers/convolutional/"},{"metadata":{},"cell_type":"markdown","source":"#### Import Libraies"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adadelta","execution_count":11,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"#### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(-1, 28, 28)\nX_train = np.expand_dims(X_train, axis=1)\nX_val = X_val.reshape(-1, 28, 28)\nX_val = np.expand_dims(X_val, axis=1)\ny_train = to_categorical(y_train, 10)\ny_val = to_categorical(y_val, 10)\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"(33600, 1, 28, 28)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Model Building and Training\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def CNN_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(1,28,28), data_format='channels_first'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10, activation='softmax'))\n    return model","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nCNN = CNN_model()\n\nCNN.compile(loss=categorical_crossentropy,\n              optimizer=Adadelta(),\n              metrics=['accuracy'])\n\nCNN.fit(X_train, y_train,\n          batch_size=128,\n          epochs=30,\n          verbose=1,\n          validation_data=(X_val, y_val))\n","execution_count":15,"outputs":[{"output_type":"stream","text":"Train on 33600 samples, validate on 8400 samples\nEpoch 1/30\n33600/33600 [==============================] - 6s 182us/step - loss: 0.7163 - accuracy: 0.7584 - val_loss: 0.1979 - val_accuracy: 0.9395\nEpoch 2/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.1699 - accuracy: 0.9508 - val_loss: 0.1061 - val_accuracy: 0.9694\nEpoch 3/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.1208 - accuracy: 0.9658 - val_loss: 0.0935 - val_accuracy: 0.9731\nEpoch 4/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0942 - accuracy: 0.9737 - val_loss: 0.0781 - val_accuracy: 0.9782\nEpoch 5/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0823 - accuracy: 0.9765 - val_loss: 0.0744 - val_accuracy: 0.9773\nEpoch 6/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0728 - accuracy: 0.9795 - val_loss: 0.0609 - val_accuracy: 0.9829\nEpoch 7/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0614 - accuracy: 0.9824 - val_loss: 0.0616 - val_accuracy: 0.9827\nEpoch 8/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0552 - accuracy: 0.9845 - val_loss: 0.0666 - val_accuracy: 0.9813\nEpoch 9/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0491 - accuracy: 0.9860 - val_loss: 0.0604 - val_accuracy: 0.9842\nEpoch 10/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0450 - accuracy: 0.9873 - val_loss: 0.0527 - val_accuracy: 0.9870\nEpoch 11/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0396 - accuracy: 0.9879 - val_loss: 0.0537 - val_accuracy: 0.9860\nEpoch 12/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0379 - accuracy: 0.9887 - val_loss: 0.0538 - val_accuracy: 0.9846\nEpoch 13/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0352 - accuracy: 0.9895 - val_loss: 0.0564 - val_accuracy: 0.9851\nEpoch 14/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0332 - accuracy: 0.9904 - val_loss: 0.0590 - val_accuracy: 0.9857\nEpoch 15/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0307 - accuracy: 0.9909 - val_loss: 0.0540 - val_accuracy: 0.9854\nEpoch 16/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0272 - accuracy: 0.9921 - val_loss: 0.0652 - val_accuracy: 0.9843\nEpoch 17/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0271 - accuracy: 0.9920 - val_loss: 0.0579 - val_accuracy: 0.9858\nEpoch 18/30\n33600/33600 [==============================] - 4s 108us/step - loss: 0.0265 - accuracy: 0.9921 - val_loss: 0.0545 - val_accuracy: 0.9869\nEpoch 19/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0231 - accuracy: 0.9931 - val_loss: 0.0630 - val_accuracy: 0.9857\nEpoch 20/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0236 - accuracy: 0.9929 - val_loss: 0.0663 - val_accuracy: 0.9855\nEpoch 21/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.0661 - val_accuracy: 0.9852\nEpoch 22/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0210 - accuracy: 0.9941 - val_loss: 0.0594 - val_accuracy: 0.9858\nEpoch 23/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0222 - accuracy: 0.9935 - val_loss: 0.0520 - val_accuracy: 0.9877\nEpoch 24/30\n33600/33600 [==============================] - 4s 105us/step - loss: 0.0180 - accuracy: 0.9946 - val_loss: 0.0567 - val_accuracy: 0.9882\nEpoch 25/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0165 - accuracy: 0.9955 - val_loss: 0.0574 - val_accuracy: 0.9874\nEpoch 26/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0177 - accuracy: 0.9951 - val_loss: 0.0582 - val_accuracy: 0.9880\nEpoch 27/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.0614 - val_accuracy: 0.9867\nEpoch 28/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0670 - val_accuracy: 0.9873\nEpoch 29/30\n33600/33600 [==============================] - 4s 106us/step - loss: 0.0159 - accuracy: 0.9956 - val_loss: 0.0563 - val_accuracy: 0.9881\nEpoch 30/30\n33600/33600 [==============================] - 4s 107us/step - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.0621 - val_accuracy: 0.9873\nCPU times: user 1min 58s, sys: 15.6 s, total: 2min 14s\nWall time: 1min 53s\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7fbe9cc69128>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{},"cell_type":"markdown","source":"Convolutional Neural Network scored the highest on validation accuracy, so we will use that model to train on the whole trianing set and make prediction on the test set."},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.iloc[:, 1:]\ny_train = train.iloc[:, 0]\nX_test = test","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(normalization(X_train))\ny_train = np.array(y_train)\nX_test = np.array(normalization(X_test))","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(-1, 28, 28)\nX_train = np.expand_dims(X_train, axis=1)\nX_test = X_test.reshape(-1, 28, 28)\nX_test = np.expand_dims(X_test, axis=1)\ny_train = to_categorical(y_train, 10)","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"CNN = CNN_model()\n\nCNN.compile(loss=categorical_crossentropy,\n              optimizer=Adadelta(),\n              metrics=['accuracy'])\n\nCNN.fit(X_train, y_train,\n          batch_size=128,\n          epochs=30,\n          verbose=1)","execution_count":19,"outputs":[{"output_type":"stream","text":"Epoch 1/30\n42000/42000 [==============================] - 5s 123us/step - loss: 0.6308 - accuracy: 0.7866\nEpoch 2/30\n42000/42000 [==============================] - 5s 111us/step - loss: 0.1454 - accuracy: 0.9581\nEpoch 3/30\n42000/42000 [==============================] - 5s 111us/step - loss: 0.1027 - accuracy: 0.9702\nEpoch 4/30\n42000/42000 [==============================] - 5s 110us/step - loss: 0.0846 - accuracy: 0.9762\nEpoch 5/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0694 - accuracy: 0.9799\nEpoch 6/30\n42000/42000 [==============================] - 5s 125us/step - loss: 0.0630 - accuracy: 0.9820\nEpoch 7/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0549 - accuracy: 0.9844\nEpoch 8/30\n42000/42000 [==============================] - 5s 109us/step - loss: 0.0486 - accuracy: 0.9855\nEpoch 9/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0456 - accuracy: 0.9871\nEpoch 10/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0410 - accuracy: 0.9879\nEpoch 11/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0369 - accuracy: 0.9896\nEpoch 12/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0350 - accuracy: 0.9904\nEpoch 13/30\n42000/42000 [==============================] - 5s 107us/step - loss: 0.0318 - accuracy: 0.9910\nEpoch 14/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0293 - accuracy: 0.9914\nEpoch 15/30\n42000/42000 [==============================] - 5s 112us/step - loss: 0.0272 - accuracy: 0.9915\nEpoch 16/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0280 - accuracy: 0.9920\nEpoch 17/30\n42000/42000 [==============================] - 5s 109us/step - loss: 0.0243 - accuracy: 0.9931\nEpoch 18/30\n42000/42000 [==============================] - 5s 109us/step - loss: 0.0216 - accuracy: 0.9934\nEpoch 19/30\n42000/42000 [==============================] - 5s 110us/step - loss: 0.0216 - accuracy: 0.9933\nEpoch 20/30\n42000/42000 [==============================] - 5s 107us/step - loss: 0.0232 - accuracy: 0.9933\nEpoch 21/30\n42000/42000 [==============================] - 5s 107us/step - loss: 0.0205 - accuracy: 0.9935\nEpoch 22/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0175 - accuracy: 0.9946\nEpoch 23/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0187 - accuracy: 0.9945\nEpoch 24/30\n42000/42000 [==============================] - 4s 107us/step - loss: 0.0182 - accuracy: 0.9943\nEpoch 25/30\n42000/42000 [==============================] - 5s 109us/step - loss: 0.0161 - accuracy: 0.9952\nEpoch 26/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0180 - accuracy: 0.9947\nEpoch 27/30\n42000/42000 [==============================] - 4s 107us/step - loss: 0.0165 - accuracy: 0.9950\nEpoch 28/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0146 - accuracy: 0.9957\nEpoch 29/30\n42000/42000 [==============================] - 4s 107us/step - loss: 0.0135 - accuracy: 0.9957\nEpoch 30/30\n42000/42000 [==============================] - 5s 108us/step - loss: 0.0150 - accuracy: 0.9953\n","name":"stdout"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7fbe9c475fd0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_prob = CNN.predict(X_test)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = np.argmax(prediction_prob, axis=1)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"array([2, 0, 9, ..., 3, 9, 2])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_df = {\"ImageId\":range(1, X_test.shape[0]+1), \"Label\":prediction}\nprediction_df = pd.DataFrame(prediction_df)\nprediction_df.to_csv(\"prediction.csv\", index = False)","execution_count":23,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}